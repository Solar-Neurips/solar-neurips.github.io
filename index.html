---
layout: home
title: Socially Responsible Language Modelling Research (SoLaR) 2024
subtitle: NeurIPS 2024 in-person Workshop, Vancouver
---

<hr class="small" style="border-width: 1pt; border-color: lightgray;">

<div class="contact-heading">Contact: <a href='mailto:solar-neurips@googlegroups.com'>solar-neurips@googlegroups.com</a>.</div>

<hr class="small" style="border-width: 1pt; border-color: lightgray;">


<h3 style='margin-bottom: 10pt;'>Sponsors</h3>


<div class="center">
	<div class='description' style='font-size: 11pt;'>
This workshop is sponsored by: TBD
  <!-- </div>
  <div class="logo-sponsor">
	<a href="https://futureoflife.org/"><img src="assets/img/sponsors/fli.png" style="height: 80px;"></a>
  </div>
  <div class style="font-size: 11pt;">and</div>
  <div class="logo-sponsor">
	<a href="https://www.microsoft.com/"><img src="assets/img/sponsors/microsoft.png" style="height: 50px;"></a>
  </div>
</div>  -->

<h3 style='margin-bottom: 10pt;'>Key Dates</h3>

<div class='description' style='font-size: 11pt;align: center'>

<table style='margin-bottom:10pt;margin-left:auto;margin-right:auto;'>
	<tr>
		<td> Submissions Open</td> 
		<td> TBD </td>
	</tr>
	<tr>
		<td> <b>Submission Deadline</b></td> 
		<td> August 30, 2024, AoE</td>
	</tr>
	<tr>
		<td> Acceptance Notification </td>
		<td> October 14, 2024, AoE</td>
	</tr>
	<tr>
		<td> Camera-Ready Deadline</td>
        <td> TBD </td>
	</tr>
	<tr>
		<td> Workshop Date</td>
        <td> December 14 or 15, 2024 </td>
	</tr>
</table>

<p>All deadlines are specified in <a href="https://www.timeanddate.com/time/zones/aoe" target="_blank">AoE</a> (Anywhere on Earth).
</p>
</div>

<div class='description' style='font-size: 11pt;'>

<h3 style='margin-bottom: 10pt;'>Description/Call For Papers</h3>
<p>The Socially Responsible Language Modelling Research (SoLaR) workshop at NeurIPS 2024 is an interdisciplinary gathering that aims to foster responsible and ethical research in the field of language modeling. Recognizing the significant risks and harms [33-37] associated with the development, deployment, and use of language models, the workshop emphasizes the need for researchers to focus on addressing these risks starting from the early stages of development. The workshop brings together experts and practitioners from various domains and academic fields with a shared commitment to promoting fairness, equity, accountability, transparency, and safety in language modeling research.</p>

Given the wide-ranging impacts of LMs, our workshop will welcome a broad array of submissions.
We briefly detail some specific topic areas and an illustrative selection of pertinent works:
<ul>
<li> Security and privacy concerns of LMs [13, 30, 25, 49, 55].
<li> Bias and exclusion in LMs [12, 2, 26, 53, 44].
<li> Analysis of the development and deployment of LMs, including crowdwork [42, 50], deploy-
ment protocols [52, 47], and societal impacts from deployment [10, 21].
<li> Safety, robustness, and alignment of LMs [51, 8, 35, 32, 7].
<li> Auditing, red-teaming, and evaluations of LMs [41, 40, 29, 15, 11].
<li> Examination of risks and harms from any novel input and/or output modalities that are
introduced in LMs [14, 28, 54].
<li> Transparency, explainability, interpretability of LMs [39, 17, 3, 46, 22, 38].
<li> Applications of LMs for social good, including sector-specific applications [9, 31, 16] and LMs
for low-resource languages [4, 5, 36].
<li> Perspectives from other domains that can inform socially responsible LM development and
deployment [48, 1].
</ul>
<b>We will also have a separate track, with a separate reviewer pool, for sociotechnical
submissions</b> from disciplines such as philosophy, law, and policy. We provide a brief illustrative
list of works we would welcome:
<ul>
<li> Studies on economic impacts of LMs, e.g., labor-market disruptions [18, 34].
<li>Risk assessment [33, 24, 37, 23].
<li> Regulation and governance of LMs [45, 6, 27].
<li> Philosophical examination of concepts related to alignment, safety [19, 43, 20].
</ul>
Papers from the previous iteration of SoLaR can be found <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/SoLaR" target="_blank"> here </a>. 

<h3 style='margin-bottom: 10pt;'>References</h3>
<div class='references' style='font-size:9pt'>
<p> 
[1] The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity. In AIES 2021. 
<br>
[2] Persistent Anti-Muslim Bias in Large Language Models. In AIES 2021.
<br>
[3] Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation. In ICLR 2022. 
<br>
[4] A Few Thousand Translations Goa Long Way! Leveraging Pre-trained Models for African News Translation. In NAACL 2022.
<br>
[5] MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition. In EMNLP 2022.
<br>
[6] Managing Emerging Risks to Public Safety, Sept. 2023. URL http://arxiv.org/abs/2307.03718.
<br>
[7] Foundational challenges in assuring alignment and safety of large
language models. arXiv preprint arXiv:2404.09932, 2024.
<br>
[8] Training a Helpful and Harmless Assistant with Reinforcement Learning from Human
Feedback, Apr. 2022. URL http://arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs]
<br>
[9] Fine-tuning language models to find agreement among humans with diverse preferences. In A. H. Oh,
A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing
Systems, 2022.
<br>
[10] On the Dangers of Stochastic
Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on
Fairness, Accountability, and Transparency, FAccT ’21.
<br>
[11] Ai auditing: The broken bus
on the road to ai accountability. arXiv preprint arXiv:2401.14462, 2024
<br>
[12] Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets. In Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1004–1015,
Online, Aug. 2021.
<br>
[13] What Does it Mean for a
Language Model to Preserve Privacy? In 2022 ACM Conference on Fairness, Accountability,
and Transparency. ACM, June 2022. 
<br>
[14] Are aligned neural networks adversarially aligned? Advances in
Neural Information Processing Systems, 36, 2023.
<br>
[15] Black-box access is insufficient for rigorous ai audits. arXiv
preprint arXiv:2401.14446, 2024.
<br>
[16] Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings.
In Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-
pers), pages 2970–3005, Minneapolis, Minnesota, June 2019
<br>
[17] Towards A Rigorous Science of Interpretable Machine Learning,
Mar. 2017. URL http://arxiv.org/abs/1702.08608. 
<br>
[18] GPTs are GPTs: An Early Look at the
Labor Market Impact Potential of Large Language Models, Aug. 2023. arXiv: 2303.10130 
<br>
[19] Artificial intelligence, values, and alignment. Minds and machines, 30(3):411–437,
2020. Publisher: Springer.
<br>
[20] he ethics of advanced AI assistants. arXiv preprint
arXiv:2404.16244, 2024.
<br>
[21] Predictability and Surprise in Large Generative Models. In 2022 ACM Conference on
Fairness, Accountability, and Transparency, FAccT ’22, pages 1747–1764, New York, NY,
USA, June 2022. Association for Computing Machinery.
<br>
[22] Datasheets for datasets. Communications of the ACM, 64(12):86–92, Dec. 2021.
<br>
[23] The false promise of risk assessments. In Proceedings of the 2020 Conference on
Fairness, Accountability, and Transparency. ACM, Jan. 2020. 
<br>
[24] Algorithmic Risk Assessments Can Alter Human Decision-Making
Processes in High-Stakes Government Contexts. Proceedings of the ACM on Human-Computer
Interaction, 5(CSCW2):418:1–418:33, Oct. 2021.
<br>
[25] Predictability and Surprise in Large Generative Models. In 2022 ACM Conference on
Fairness, Accountability, and Transparency, FAccT ’22, pages 1747–1764, New York, NY,
USA, June 2022.
<br>
[26] Datasheets for datasets. Communications of the ACM, 64(12):86–92, Dec. 2021
<br>
[27] The false promise of risk assessments. In Proceedings of the 2020 Conference on
Fairness, Accountability, and Transparency. ACM, Jan. 2020. 
<br>
[28] Algorithmic Risk Assessments Can Alter Human Decision-Making
Processes in High-Stakes Government Contexts. Proceedings of the ACM on Human-Computer
Interaction, 5(CSCW2):418:1–418:33, Oct. 202
<br>
[29] Not what you’ve
signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt
Injection, May 2023.
<br>
[30] ias runs deep: Implicit reasoning biases in persona-assigned llms. arXiv preprint
arXiv:2311.04892, 2023.
<br>
[31] A Real-World
WebAgent with Planning, Long Context Understanding, and Program Synthesis, Feb. 2024.
URL http://arxiv.org/abs/2307.12856. arXiv:2307.12856 [cs]
<br>
[32] The Future of AI Governance, Apr. 2023.
URL http://arxiv.org/abs/2304.04914
<br>
[33] Uncovering bias in large
vision-language models with counterfactuals. arXiv preprint arXiv:2404.00166, 2024
<br>
[34] Automatically Auditing Large
Language Models via Discrete Optimization, Mar. 2023. URL http://arxiv.org/abs/2303.
04381.
<br>
[35] Deduplicating Training Data Mitigates Privacy Risks
in Language Models. In Proceedings of the 39th International Conference on Machine Learning, pages 10697–10707. PMLR, June 2022.
<br>
[36] ChatGPT for good? On opportunities and challenges of large language mod-
els for education. Learning and Individual Differences, 103:102274, Apr. 2023.
<br>
[37] Alignment of Language Agents, Mar. 2021. URL http://arxiv.org/abs/2103.14659.
<br>
</div>
