---
layout: page
title: Submission Guide
subtitle: 
---

<h3 style='margin-bottom: 10pt;'>Call for Papers</h3>

<div class='description' style='font-size: 11pt;'>

The 2nd Socially Responsible Language Modelling Research (SoLaR) workshop at NeurIPS 2024
is soliciting papers on the socially responsible development and deployment of language models
(and related systems such as multimodal models or LM-based agents), as well as their impacts.
The workshop will welcome experts and practitioners from various domains, with a shared commit-
ment to promoting fairness, equity, accountability, transparency, and safety in language modeling
research.
Given the wide-ranging impacts of LMs, our workshop will welcome a broad array of submissions.
We briefly detail some specific topic areas and an illustrative selection of pertinent works:
<ul>
<li> Security and privacy concerns of LMs [13, 30, 25, 49, 55].
<li> Bias and exclusion in LMs [12, 2, 26, 53, 44].
<li> Analysis of the development and deployment of LMs, including crowdwork [42, 50], deploy-
ment protocols [52, 47], and societal impacts from deployment [10, 21].
<li> Safety, robustness, and alignment of LMs [51, 8, 35, 32, 7].
<li> Auditing, red-teaming, and evaluations of LMs [41, 40, 29, 15, 11].
<li> Examination of risks and harms from any novel input and/or output modalities that are
introduced in LMs [14, 28, 54].
<li> Transparency, explainability, interpretability of LMs [39, 17, 3, 46, 22, 38].
<li> Applications of LMs for social good, including sector-specific applications [9, 31, 16] and LMs
for low-resource languages [4, 5, 36].
<li> Perspectives from other domains that can inform socially responsible LM development and
deployment [48, 1].
</ul>
<b>We will also have a separate track, with a separate reviewer pool, for sociotechnical
submissions</b> from disciplines such as philosophy, law, and policy. We provide a brief illustrative
list of works we would welcome:
<ul>
<li> Studies on economic impacts of LMs, e.g., labor-market disruptions [18, 34].
<li>Risk assessment [33, 24, 37, 23].
<li> Regulation and governance of LMs [45, 6, 27].
<li> Philosophical examination of concepts related to alignment, safety [19, 43, 20].
Papers from the previous iteration of SoLaR can be found
</ul>
<h3 style='margin-bottom: 10pt;'>Submission Instructions</h3>

<div class='description' style='font-size: 11pt;'>
 <p>Submission should be made on OpenReview (link TBD).</p>
 <!-- <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/SoLaR" target="_blank">OpenReview</a>-->
<p> Submissions should be anonymised papers up to 5 pages (appendices can be added to the main PDF); excluding references. Authors are encouraged to include a "Social Impacts Statement" that highlights "potential broader impact of their work, including its ethical aspects and future societal consequences". You must format your submission using the <a href="https://media.neurips.cc/Conferences/NeurIPS2024/Styles.zip" target="_blank"> NeurIPS 2024 LaTeX style file </a> or <a href="https://github.com/ICLR/Master-Template/raw/master/iclr2024.zip " target="_blank"> ICLR 2025 LaTeX style file </a>. Reviews will be double-blind, with at least two reviewers assigned to each paper.</p> 

<p>We welcome various types of papers including scientific papers, position papers, policy papers, and works in progress. Scientific papers must not have appeared at an archival venue before, however, non-scientific papers that have appeared in archival venues outside main machine learning venues are welcomed for submission.</p>
<p>All accepted papers will be available on the workshop website, but no formal workshop proceedings will be published.</p>


 <p>We will shortly provide a link to submit your paper on the OpenReview portal.</p>
<p> We have also provided an optional <a href="https://www.overleaf.com/latex/templates/socially-responsible-language-modelling-research-solar-at-neurips-2024-template/ckhxbtxyghkf" target="_blank">Overleaf template</a> for your convenience.</p>
<p> <b>We will have two tracks</b>: 1) a technical track and 2) a sociotechnical track, with a separate
reviewer pool, for submissions from disciplines such as philosophy, law, or policy.</p>

<p> The maximum length is 5 pages, excluding references.
We welcome various types of papers including scientific papers, position papers, policy papers,
and works in progress. Scientific papers must not have appeared at an archival venue before, <b>but
concurrent submissions to the main NeurIPS 2024 conference are acceptable</b>. Non-
scientific papers that have appeared in archival venues outside main machine-learning venues are
welcomed for submission.
All submitted papers will undergo a double-blind review process.
All accepted papers will be available on the workshop website, but no formal workshop proceedings
will be published.</p>


<h3 style='margin-bottom: 10pt;'>Key Dates</h3>

<div class='description' style='font-size: 11pt;align: center'>

<table style='margin-bottom:10pt;margin-left:auto;margin-right:auto;'>
	<tr>
		<td> <b>Submission Deadline</b></td> 
		<td> August 30, 2024, AoE</td>
	</tr>
	<tr>
		<td> Acceptance Notification </td>
		<td> October 14, 2024, AoE</td>
	</tr>
	<tr>
		<td> Camera-Ready Deadline</td>
        <td> TBD </td>
	</tr>
	<tr>
		<td> Workshop Date</td>
        <td> December 14 or 15, 2024 </td>
	</tr>
</table>

<p>All deadlines are specified in <a href="https://www.timeanddate.com/time/zones/aoe" target="_blank">AoE</a> (Anywhere on Earth).
</p>
</div>


<h3 style='margin-bottom: 10pt;'>FAQ</h3>
<p>Q: Can we submit a paper that will also be submitted to ICLR 2025?</p>
<p>A: Yes</p>

<p>Q: Will the reviews be made available to authors?</p>
<p>A: Yes.</p>

<p>Q: I have a question not addressed here, whom should I contact?</p>
<p>A: Email organizers at solar-neurips@googlegroups.com </p>
</div>

<h3 style='margin-bottom: 10pt;'>References</h3>
<div class='references' style='font-size:9pt'>
<p> 
[1] The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity. In AIES 2021. 
<br>
[2] Persistent Anti-Muslim Bias in Large Language Models. In AIES 2021.
<br>
[3] Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation. In ICLR 2022. 
<br>
[4] A Few Thousand Translations Goa Long Way! Leveraging Pre-trained Models for African News Translation. In NAACL 2022.
<br>
[5] MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition. In EMNLP 2022.
<br>
[6] Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Apr. 2022. URL http://arxiv.org/abs/2204.05862. arXiv:2204.05862.
<br>
[7] Fine-tuning language models to find agreement among humans with diverse preferences. In NeurIPS 2022.
<br>
[8] Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 2021.
<br>
[9] Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets. In roceedings of the 59th Annual Meeting of the Association for Computational Linguistic, August 2021.
<br>
[10] What Does it Mean for a Language Model to Preserve Privacy? In 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM, June 2022.
<br>
[11] Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings. In 2019 NAACL, June 2019.
<br>
[12] Towards A Rigorous Science of Interpretable Machine Learning, Mar. 2017. URL http://arxiv.org/abs/1702.08608.
<br>
[13] Predictability and Surprise in Large Generative Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22, June 2022.
<br>
[14] Datasheets for datasets. Communications of the ACM, 64(12):86–92, Dec. 2021. 
<br>
[15] Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection, May 2023. URL http://arxiv.org/abs/2302.12173. arXiv:2302.12173.
<br>
[16] Automatically Auditing Large Language Models via Discrete Optimization, Mar. 2023. URL http://arxiv.org/abs/2303.04381. arXiv:2303.04381.
<br>
[17] Deduplicating Training Data Mitigates Privacy Risks in Language Models. In ICML 2022.
<br>
[18] ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103:102274, Apr. 2023. URL https://www.sciencedirect.com/science/
article/pii/S1041608023000195.
<br>
[19] Alignment of Language Agents, Mar. 2021. URL http://arxiv.org/abs/2103.14659. arXiv:2103.14659.
<br>
[20] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In ACL 2022.
<br>
[21] Challenges of language technolo-gies for the indigenous languages of the Americas. In ACL 2018.
<br>
[22] Model Cards for Model Reporting. In FAacT 2019.
<br>
[23] In-context Learning and Induction Heads. Transformer Circuits Thread, 2022.
<br>
[24] Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark, Apr. 2023. URL http://arxiv.org/abs/2304.03279. arXiv:2304.03279.
<br>
[25]iscovering Language Model Behaviors with Model-Written Evaluations, Dec. 2022. URL http://arxiv.org/abs/2212.09251. arXiv:2212.09251.
<br>
[26] The Coloniality of Data Work in Latin America. In AIES 2021.
<br>
[27] Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, May 2019.
<br>
[28] Structured access: an emerging paradigm for safe AI deployment, Apr. 2022. URL http://arxiv.org/abs/2201.05159. arXiv:2201.05159.
<br>
[29] The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse? In AIES ’20Feb. 2020. 
<br>
[30] Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing. In NAACL 2021.
<br>
[31] Defining and Characterizing Reward Hacking. In NeurIPS, 2022.
<br>
[32] The Gradient of Generative AI Release: Methods and Considerations, Feb. 2023. URL http://arxiv.org/abs/2302.04844. arXiv:2302.04844.
<br>
[33] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In FAccT 2021.
i<br>
[34] Taxonomy of Risks posed by Language Models. In FAccT 2022.
<br>
[35] Sociotechnical Harms: Scoping a Taxonomy for Harm Reduction, Oct. 2022. URL http://arxiv.org/abs/2210.05791.
<br>
[36] Predictability and Surprise in Large Generative Models. In FAccT 2022.
<br>
[37] Emergent Abilities of Large Language Models. In TMLR 2022.
</div>

