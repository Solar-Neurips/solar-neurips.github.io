---
layout: page
title: Call for Papers
subtitle: 
---

<h3 style='margin-bottom: 10pt;'>Key Dates</h3>

<div class='description' style='font-size: 11pt;align: center'>

<table style='margin-bottom:10pt;'>
	<tr>
		<td> <b>Submission Deadline</b></td> 
		<td> 26 September, 2022 </td>
	</tr>
	<tr>
		<td> Acceptance Notification </td>
		<td> 14 October, 2022 </td>
	</tr>
	<tr>
		<td> Camera-Ready Deadline</td>
		<td> 16 November, 2022</td>
	</tr>
	<tr>
		<td> Workshop Date</td>
		<td> 2 December, 2022</td>
	</tr>
</table>

<p>All deadlines are specified in <a href="https://www.timeanddate.com/time/zones/aoe" target="_blank">AoE</a> (Anywhere on Earth).
</p>

</div>

<h3 style='margin-bottom: 10pt;'>Topics</h3>

<div class='description' style='font-size: 11pt;'>

As language models (LMs) revolutionize various fields, it becomes critical to ensure their respon-
sible development and deployment. This includes considering factors like alignment, security, and
fairness, while striving for bias-free, misuse-resistant, and interpretable models. Our workshop aims
to encourage a wider discussion on these societal impacts of LMs, promoting more socially conscious
and accountable LM research. We anticipate a broad array of submissions given the multi-faceted
impact of LMs. Our focus areas will include but are not limited to:

<ul>
<li>Security and privacy concerns of LMs [10, 17, 15] </li>
<li>Bias and exclusion in LMs [9, 2] </li>
<li>Analysis of the development and deployment of LMs, including crowdwork [26, 30], deployment protocols [32, 28], and societal impacts from deployment [8, 13]. </li>
<li>Safety, robustness, and alignment of LMs [31, 6, 20, 19] </li>
<li>Auditing, red-teaming, and evaluations of LMs [25, 24, 16] </li>
<li>Transparency, explainability, interpretability of LMs [23, 12, 3, 27, 14, 22] </li>
<li>Applications of LMs for social good, including sector-specific applications [7, 18, 11] and LMs </li>
<li>for low-resource languages [4, 5, 21] </li>
<li>Perspectives from other domains that can inform socially responsible LM development and
deployment [29, 1] </li>
</ul>

We also encourage sociotechnical submissions from other disciplines such as philosophy, law, and
policy, in order to foster an interdisciplinary dialogue on the societal impacts of LMs.

</div>


<h3 style='margin-bottom: 10pt;'>Submission Instructions</h3>

<div class='description' style='font-size: 11pt;'>
<p>Submission should be made on <a href="https://openreview.net/group?id=NeurIPS.cc/2022/Workshop/LaReL" target="_blank">OpenReview</a>.</p>

<p> Submissions should be anonymised papers up to 4 pages (appendices can be added to the main PDF). You must format your submission using the <a href="https://neurips.cc/Conferences/2022/PaperInformation/StyleFiles" target="_blank"> NeurIPS 2022 LaTeX style file </a>. Reviews will be double-blind, with at least two reviewers assigned to each paper.</p> 

<p>The papers should report original research, provide synthesis of previous works or develop novel environments. Short opinion and review papers are welcomed. We accept dual submission. Authors can upload concise versions of parallel submissions to other conferences such as NeurIPS main conference or ICLR. We discourage submitting to multiple NeurIPS workshops</p>

<p>All accepted papers will be available on the workshop website, but no formal workshop proceedings will be published.</p>

<p>For any questions, email us at <a href='mailto:larel.ws@gmail.com'>larel.ws@gmail.com</a>.</p>

<h3 style='margin-bottom: 10pt;'>References</h3>


<div class='references' style='font-size:9pt'>
<p> 
[1] The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat
on Academic Integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and
Society. ACM, July 2021. 
<br>
[2] Persistent Anti-Muslim Bias in Large Language Models. In
Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. ACM, July 2021.
<br>
[3] Post hoc Explanations may be Ineffec-
tive for Detecting Unknown Spurious Correlation. In International Conference on Learning
Representations, 2022. 
<br>
[4] A Few Thousand Translations Goa Long Way! Leveraging Pre-trained Models for African News Translation. In Proceedings
of the 2022 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, July 2022.
<br>
[5] MasakhaNER 2.0:
Africa-centric Transfer Learning for Named Entity Recognition. In Proceedings of the 2022
Conference on Empirical Methods in Natural Language Processing, Dec. 2022.
<br>
[6] Training a Helpful and Harmless Assistant with Reinforcement Learning from Human
Feedback, Apr. 2022. URL http://arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs].
<br>
[7] Fine-tuning language models to find agreement among humans with diverse preferences. In NeurIPS 2022.
<br>
[8] Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on
Fairness, Accountability, and Transparency, FAccT 2021.
<br>
[9] Stereotyping Norwegian
Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets. In roceedings of the 59th
Annual Meeting of the Association for Computational Linguistic, August 2021.
<br>
[10] What Does it Mean for a
Language Model to Preserve Privacy? In 2022 ACM Conference on Fairness, Accountability,
and Transparency. ACM, June 2022.
<br>
[11] Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings. In 2019 NAACL, June 2019.
<br>
[12] Towards A Rigorous Science of Interpretable Machine Learning,
Mar. 2017. URL http://arxiv.org/abs/1702.08608
<br>
[13] Predictability and Surprise in Large Generative Models. In 2022 ACM Conference on
Fairness, Accountability, and Transparency, FAccT ’22, June 2022.
<br>
[14] Datasheets for datasets. Communications of the ACM, 64(12):86–92, Dec. 2021. 
<br>
[15] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz. Not what you’ve
signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt
Injection, May 2023. URL http://arxiv.org/abs/2302.12173. arXiv:2302.12173 [cs].
3
<br>
[16] E. Jones, A. Dragan, A. Raghunathan, and J. Steinhardt. Automatically Auditing Large
Language Models via Discrete Optimization, Mar. 2023. URL http://arxiv.org/abs/2303.
04381. arXiv:2303.04381 [cs].
<br>
[17] N. Kandpal, E. Wallace, and C. Raffel. Deduplicating Training Data Mitigates Privacy Risks
in Language Models. In Proceedings of the 39th International Conference on Machine Learn-
ing, pages 10697–10707. PMLR, June 2022. URL https://proceedings.mlr.press/v162/
kandpal22a.html. ISSN: 2640-3498.
<br>
[18] ChatGPT for good? On opportunities and challenges of large language mod-
els for education. Learning and Individual Differences, 103:102274, Apr. 2023. <https://www.sciencedirect.com/science/
article/pii/S1041608023000195>.
<br>
[19] Alignment of
Language Agents, Mar. 2021. URL <http://arxiv.org/abs/2103.14659>. arXiv:2103.14659.
<br>
[20] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring How Models Mimic Human False-
hoods. In ACL 2022.
<br>
[21] Challenges of language technolo-gies for the indigenous languages of the Americas. In ACL 2018.
<br>
[22] Model Cards for Model Reporting. In FAacT 2019. <br>
<br>
[23] In-context Learning and Induction Heads. Transformer Circuits Thread, 2022.
<br>
[24] Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards
and Ethical Behavior in the MACHIAVELLI Benchmark, Apr. 2023. <http://arxiv.
org/abs/2304.03279>. arXiv:2304.03279 [cs].
<br>
[25]iscovering Language Model Behaviors with Model-Written Evaluations, Dec. 2022. <http://arxiv.org/abs/2212.09251>. arXiv:2212.09251.
<br>
[26] The Coloniality of Data Work in Latin America. In AIES 2021.
<br>
[27] Stop explaining black box machine learning models for high stakes decisions
and use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, May 2019.
<br>
[28] Structured access: an emerging paradigm for safe AI deployment, Apr. 2022.
<http://arxiv.org/abs/2201.05159>. arXiv:2201.05159.
<br>
[29] The Offense-Defense Balance of Scientific Knowledge: Does Pub-
lishing AI Research Reduce Misuse? In AIES ’20Feb. 2020. 
<br>
[30] Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing. In NAACL 2021.
<br>
[31] Defining and Characterizing Reward Hacking. In NeurIPS, 2022.
<br>
[32] The Gradient of Generative AI Release: Methods and Considerations, Feb. 2023.
<http://arxiv.org/abs/2302.04844>. arXiv:2302.04844.


</div>

